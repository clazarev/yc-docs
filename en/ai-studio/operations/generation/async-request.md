---
title: How to send asynchronous requests to the text generation API
description: Follow this tutorial to learn how to send asynchronous requests to models in {{ gpt-lite }} and {{ gpt-pro }}.
---

# Sending an asynchronous request

You can send requests to text generation models in [asynchronous mode](../../concepts/index.md#working-mode). In response to an asynchronous request, the model will return an [operation object](../../../api-design-guide/concepts/operation.md) containing the operation ID you can use to [follow up the operation's progress](../../../api-design-guide/concepts/operation.md#monitoring) and get the result once the generation is complete. Use this mode if you do not need an urgent response, since asynchronous requests take longer to complete than [synchronous](./create-prompt.md) ones.

## Getting started {#before-begin}

{% list tabs group=programming_language %}

- SDK {#sdk}

  To use the examples of requests using SDK:

  {% include [sdk-before-begin-ai-langmodel-user](../../../_includes/ai-studio/sdk-before-begin-ai-langmodel-user.md) %}

- cURL {#curl}

  {% include notitle [ai-before-beginning](../../../_includes/ai-studio/yandexgpt/ai-before-beginning.md) %}

  {% include [curl](../../../_includes/curl.md) %}

{% endlist %}

## Send a request to the model {#request}

{% list tabs group=programming_language %}

- SDK {#sdk}

  When using [{{ ml-sdk-full-name }}](../../sdk/index.md), you can configure your code to wait for the operation to complete and return the response. To do this, use either the `sleep` function of the `time` module or the `wait` method. The example utilizes both of these methods one by one.

  1. Create a file named `generate-deferred.py` and paste the following code into it:

      {% include [yandexgpt-deferred-sdk](../../../_includes/ai-studio/examples/yandexgpt-deferred-sdk.md) %}

      Where:

      {% note info %}

      {% include [sdk-input-format](../../../_includes/ai-studio/sdk-input-format.md) %}

      {% endnote %}

      * `messages_1` and `messages_2`: Arrays of messages providing the context for the model, each used for a different method of getting an asynchronous request result:

          * `role`: Message sender's role:

              * `user`: Used for sending user messages to the model.
              * `system`: Used to set the query context and define the model's behavior.
              * `assistant`: Used for responses generated by the model. In chat mode, the model's responses tagged with the `assistant` role are included in the message to save the conversation context. Do not send user messages with this role.

      * `text`: Message text.

      {% include [sdk-code-legend](../../../_includes/ai-studio/examples/sdk-code-legend.md) %}

      {% include [yandexgpt-model-version-sdk-info](../../../_includes/ai-studio/yandexgpt/yandexgpt-model-version-sdk-info.md) %}

  1. Run the created file:

      ```bash
      python3 generate-deferred.py
      ```

      Result:

      ```text
      Variant 1:
      GPTModelResult(alternatives=(Alternative(role='assistant', text='Ламинат подойдёт для укладки на кухне или в детской комнате – он не боится влаги и механических повреждений благодаря защитному слою из облицованных меламиновых плёнок толщиной 0,2 мм и обработанным воском замкам.', status=<AlternativeStatus.FINAL: 3>),), usage=Usage(input_text_tokens=74, completion_tokens=46, total_tokens=120), model_version='23.10.2024')
      Variant 2:
      GPTModelResult(alternatives=(Alternative(role='assistant', text='Errors will not correct themselves.\n\nErors → errors.', status=<AlternativeStatus.FINAL: 3>),), usage=Usage(input_text_tokens=32, completion_tokens=16, total_tokens=48), model_version='23.10.2024')
      ```

      The code waits for the result of the first method and then of the second one.

- cURL {#curl}

  {% include [curl](../../../_includes/curl.md) %}
  
  {% include [bash-windows-note-single](../../../_includes/translate/bash-windows-note-single.md) %}

  1. Create a file with the request body, e.g., `body.json`:
  
     ```json
     {
       "modelUri": "gpt://<folder_ID>/yandexgpt",
       "completionOptions": {
         "stream": false,
         "temperature": 0.1,
         "maxTokens": "2000",
         "reasoningOptions": {
           "mode": "DISABLED"
         }
       },
       "messages": [
         {
           "role": "system",
           "text": "Translate the text"
         },
         {
           "role": "user",
           "text": "To be, or not to be: that is the question."
         }
       ]
     }
     ```
  
     {% include [api-parameters](../../../_includes/ai-studio/yandexgpt/api-parameters.md) %}
  
  1. Send a request to the model by running this command:
  
     ```bash
     export FOLDER_ID=<folder_ID>
     export IAM_TOKEN=<IAM_token>
     curl \
       --request POST \
       --header "Content-Type: application/json" \
       --header "Authorization: Bearer ${IAM_TOKEN}" \
       --header "x-folder-id: ${FOLDER_ID}" \
       --data "@<path_to_JSON_file>" \
       "https://llm.{{ api-host }}/foundationModels/v1/completionAsync"
     ```
  
     Where:
  
     * `FOLDER_ID`: ID of the folder for which your account has the `{{ roles-yagpt-user }}` role or higher.
     * `IAM_TOKEN`: IAM token you got [before you started](#before-begin).
  
     In the response, the service will return the `operation` object:
  
     ```json
     {
       "id": "d7qi6shlbvo5********",
       "description": "Async GPT Completion",
       "createdAt": "2023-11-30T18:31:32Z",
       "createdBy": "aje2stn6id9k********",
       "modifiedAt": "2023-11-30T18:31:33Z",
       "done": false,
       "metadata": null
     }
     ```
  
     Save the operation `id` you get in the response.
  
  1. Send a request to get the operation result:
  
      ```bash
      curl \
        --request GET \
        --header "Authorization: Bearer ${IAM_TOKEN}" \
        https://{{ api-host-operation }}/operations/<operation_ID>
      ```
  
      Result example:
  
      ```bash
      {
        "done": true,
        "response": {
          "@type": "type.googleapis.com/yandex.cloud.ai.foundation_models.v1.CompletionResponse",
          "alternatives": [
            {
              "message": {
                "role": "assistant",
                "text": "To be, or not to be, that is the question."
              },
              "status": "ALTERNATIVE_STATUS_FINAL"
            }
          ],
          "usage": {
            "inputTextTokens": "31",
            "completionTokens": "10",
            "totalTokens": "41"
          },
          "modelVersion": "18.01.2024"
        },
        "id": "d7qo21o5fj1u********",
        "description": "Async GPT Completion",
        "createdAt": "2024-05-12T18:46:54Z",
        "createdBy": "ajes08feato8********",
        "modifiedAt": "2024-05-12T18:46:55Z"
      }
      ```

{% endlist %}

#### See also {#see-also}

* [{#T}](../../concepts/generation/index.md)
* Examples of working with {{ ml-sdk-name }} on [GitHub](https://github.com/yandex-cloud/yandex-cloud-ml-sdk/tree/master/examples/sync/completions)