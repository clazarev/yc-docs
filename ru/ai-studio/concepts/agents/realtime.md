---
title: Голосовые агенты в {{ foundation-models-full-name }}
description: Голосовые агенты — это инструмент, позволяющий создавать на основе искусственного интеллекта приложения, которые взаимодействуют с пользователем как в текстовом, так и в голосовом формате.
---

# Голосовые агенты

_Голосовые агенты_ — это инструмент {{ foundation-models-full-name }}, позволяющий создавать на основе искусственного интеллекта приложения, которые взаимодействуют с пользователем как в текстовом, так и в голосовом формате.

Голосовые агенты поддерживают двусторонний обмен сообщениями: клиент отправляет события с аудио- или текстовыми инструкциями, а сервер возвращает ответы по мере готовности. Такой потоковый режим позволяет отображать частичные ответы сразу, не дожидаясь завершения обработки и обеспечивая естественное течение диалога.

Голосовые агенты могут применяться для реализации следующих сценариев:

* **Подсказки оператору**. Голосовой агент в режиме реального времени анализирует голосовое или текстовое общение оператора с клиентом и предлагает оператору готовый ответ или ссылку на инструкцию. Это позволяет оператору отвечать на вопросы быстрее и точнее.
* **Голосовой агент**. Современная замена голосовым ботам. Голосовой агент может принимать заказы, отвечать на вопросы в службу поддержки. При этом агент разговаривает без задержек, как живой оператор.
* **Автоматическая суммаризация звонка**. Голосовой агент в режиме реального времени анализирует аудиопоток и выделяет ключевые моменты (темы, договоренности, дальнейшие шаги). К концу беседы формирует краткое резюме и список задач, которые отправляет в [CRM](https://ru.wikipedia.org/wiki/Система_управления_взаимоотношениями_с_клиентами)-систему или в чат.

## {{ realtime-api }} {#realtime-api}

В {{ ai-studio-name }} голосовые агенты можно создавать с помощью _{{ realtime-api }}_ — событийно-ориентированного интерфейса для голосового взаимодействия сервера с клиентом в режиме реального времени через транспорт на базе [WebSocket](https://ru.wikipedia.org/wiki/WebSocket).

### Модель {#model}

Для обработки запросов пользователя {{ realtime-api }} использует специальную мультимодальную модель, которая подходит для интерактивных голосовых и смешанных (голос + текст) сценариев с минимальной задержкой:

**Модель и URI** | **Контекст** | **[Режимы работы](../index.md#working-mode)**
--- | --- | ---
**{{ realtime-model }}**</br>`gpt://<идентификатор_каталога>/{{ realtime-model }}` | 32 000 | Синхронный

Модель разработана специально для работы с русским языком и подойдет для создания голосовых ассистентов, чатов и приложений, где важен живой, естественный диалог.

### Сессии {#sessions}

Контекст взаимодействия между клиентом и сервером сохраняется в _сессиях_. Сессия содержит историю диалога и параметры конфигурации — системный промт модели, выбранный голос для синтеза речи, ожидаемые модальности (текст или речь).

Сессия создается один раз при установлении WebSocket-соединения и действует до закрытия этого соединения или до истечения времени жизни сессии.

Время жизни сессии составляет пять минут, но при необходимости сессия может быть продлена до десяти минут. Параметры конфигурации сессии можно изменять в процессе диалога. Например, вы можете обновлять системный промт, изменять голос синтеза или набор модальностей.

Чтобы продолжить работу после завершения текущей сессии, нужно создать новую сессию.

### События {#events}

_События_ — это основной механизм обмена данными в {{ realtime-api }}. Каждое взаимодействие клиента с сервером описывается в виде события, содержащего обязательное поле `type` — _[тип события](#types)_, указывающий на его назначение.

Клиент отправляет события, чтобы передать данные, инструкции или команды (например, создать новый ответ, загрузить аудио или изменить параметры сессии). Сервер отвечает событиями, которые содержат промежуточные или финальные результаты, а также уведомления о состоянии.

Обмен событиями двусторонний и асинхронный: клиент может отправлять на сервер новые события, не дожидаясь, пока сервер вернет результат предыдущей задачи. Такой подход позволяет обрабатывать ответы в потоковом режиме и реагировать на них сразу, без необходимости ждать завершения всей обработки.

Каждое событие передается как отдельный [JSON](https://ru.wikipedia.org/wiki/JSON)-объект по открытому WebSocket-соединению. Клиент должен уметь принимать и обрабатывать события в режиме реального времени. Кроме того, необходимо учитывать, что ответ может приходить по частям: сначала в виде дельт (частичных данных), а затем — финальным сообщением о завершении.

#### Типы событий {#types}

{{ realtime-api }} поддерживает следующие типы событий:

* `session.update` — обновление параметров сессии. Например: изменение голоса синтеза или системной инструкции для модели.
* `input_audio_buffer.append` — передача фрагмента аудиоданных (в формате [PCM](https://ru.wikipedia.org/wiki/Импульсно-кодовая_модуляция), mono, 24 kHz, 16-bit в кодировке [Base64](https://{{ lang }}.wikipedia.org/wiki/Base64)).
* `input_audio_buffer.commit` — завершение передачи аудио.
* `response.create` — запуск генерации нового ответа модели.
* `response.output_text.delta` — фрагмент ответа в форме текста (streaming).
* `response.output_audio.delta` — фрагмент ответа в форме аудио (PCM, Base64).
* `response.output_audio.done` — последний фрагмент ответа в форме аудио.
* `response.done` — завершение генерации ответа модели.
* `error` — сообщение об ошибке.


### Схема взаимодействия клиента и сервера в {{ realtime-api }} {#interaction-scheme}

Взаимодействие клиента и сервера в {{ realtime-api }} реализуется по следующей схеме:

1. Клиент отправляет аудиоданные, используя [событие](#events) `input_audio_buffer.append`. Можно отправить несколько фрагментов подряд.
1. Клиент завершает передачу событием `input_audio_buffer.commit`.
1. Клиент создает запрос на генерацию нового ответа модели с помощью события `response.create`, содержащего инструкции и параметры ответа.
1. Сервер возвращает потоковые события:

    * `response.output_text.delta` — фрагменты текста, если ответ запрошен в форме текста;
    * `response.output_audio.delta` — фрагменты аудио, если ответ запрошен в форме синтезированной речи.
1. После завершения генерации от сервера поступает событие `response.done` — сигнал о том, что новых данных больше не будет.
1. Клиент может сразу отправить новое событие `input_audio_buffer.append` или `response.create`, не разрывая соединение.

Пример создания голосового агента с помощью {{ realtime-api }} см. в разделе [{#T}](../../operations/agents/create-voice-agent.md).

{% note tip %}

* Вы можете динамически изменять стиль ответов или системные инструкции без пересоздания [сессии](#sessions).
* Подписывайтесь на [события](#types) типа `error`, корректно завершайте или перезапускайте циклы, чтобы приложение не зависло.
* Реализуйте механизм буферизации на стороне клиента, так как сервер может присылать данные быстрее, чем клиент будет их обрабатывать.

{% endnote %}

#### См. также {#see-also}

* [{#T}](../../operations/agents/create-voice-agent.md)