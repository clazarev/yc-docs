# Плагины {{ OS }}

{{ mos-name }} содержит ряд [предустановленных плагинов]({{ os.docs }}/install-and-configure/plugins/#bundled-plugins). Также при создании или изменении кластера в {{ mos-short-name }} вы можете указать желаемый список [дополнительных плагинов](#opensearch), и они будут автоматически установлены в кластер.

## Дополнительные предустановленные плагины {#extra-preinstalled}

* repository-s3

    Добавляет поддержку [AWS S3](https://aws.amazon.com/s3/) в качестве репозитория снапшотов.

## Дополнительные плагины {{ OS }} {#opensearch}

Полный список поддерживаемых дополнительных плагинов:

* analysis-icu

    Добавляет модуль Lucene ICU с расширенной поддержкой Unicode и использованием библиотек [ICU](https://icu.unicode.org/). Модуль предоставляет улучшенный анализ азиатских языков, нормализацию Unicode, преобразование регистра Unicode, поддержку сопоставления и транслитерацию.

* analysis-kuromoji

    Добавляет модуль анализа Lucene kuromoji для японского языка.

* analysis-nori

    Добавляет модуль анализа Lucene nori для корейского языка. Использует словарь [mecab-ko-dic](https://bitbucket.org/eunjeon/mecab-ko-dic/src/master/).

* analysis-phonetic

    Предоставляет фильтры лексем, которые преобразуют выражения в их фонетическое представление с помощью Soundex, Metaphone и других алгоритмов.

* analysis-smartcn

    Добавляет модуль анализа Smart Chinese от Lucene для китайского или смешанного китайско-английского текста.

* analysis-stempel

    Добавляет модуль анализа Stempel от Lucene для польского языка.

* analysis-ukrainian

    Добавляет морфологический модуль анализа UkrainianMorfologikAnalyzer от Lucene для украинского языка. Использует проект [Morfologik](https://github.com/morfologik/morfologik-stemming).

* ingest-attachment

    Извлекает вложения файлов в распространенных форматах (таких как PPT, XLS и PDF) с помощью библиотеки извлечения текста [Apache Tika™](https://tika.apache.org/).

* mapper-annotated-text

    Индексирует текст, представляющий собой комбинацию обычного текста и специальной разметки. Такая комбинация используется для идентификации объектов, таких как люди или организации.

* mapper-murmur3

    Вычисляет хеш значений полей по индексному времени и хранит их в индексе.

* mapper-size

    Предоставляет поле метаданных `_size`, которое индексирует размер в байтах исходного поля `_source`.

* repository-azure

    Добавляет поддержку [хранилища Azure Blob]({{ ms.docs }}/azure/storage/blobs/storage-blobs-introduction) в качестве репозитория [снапшотов](../../glossary/snapshot.md).

* repository-gcs

    Добавляет поддержку службы [Google Cloud Storage](https://cloud.google.com/storage/) в качестве репозитория снапшотов.

* repository-hdfs

    Добавляет поддержку файловой системы HDFS в качестве репозитория снапшотов.

* transport-nio

    Серверно-клиентская неблокируемая сетевая библиотека, созданная с помощью Netty. Плагин поддерживает только [версии {{ OS }} 2.x]({{ os.docs }}/breaking-changes/#transport-nio-plugin).

* yandex-lemmer

    Добавляет фильтр [yandex-lemmer](#yandex-lemmer), который улучшает поиск текста на русском языке по документам {{ OS }}.

Подробнее см. в [документации {{ OS }}]({{ os.docs }}/install-and-configure/plugins/#additional-plugins).

### Зачем использовать плагин yandex-lemmer {#yandex-lemmer}

Фильтр `yandex-lemmer` детальнее анализирует поисковый запрос на русском языке и делает поиск более результативным. Без этого фильтра поиск может выдать нерелевантные результаты в следующих случаях:

* Формы одного слова сильно различаются. Например, при поиске по слову `пёс` не будет найден документ со словом `пса` (родительный падеж от слова `пёс`). А при поиске по слову `идти` не будет найден документ со словом `шли` (форма прошедшего времени от слова `идти`).

* Разные по значению слова совпадают или просто схожи по написанию. В результате могут быть найдены лишние документы, которые не отвечают вашему запросу. Например, при поиске по слову `алая` будут найдены документы, которые содержат как прилагательные `алая`, так и существительные в родительном падеже от названия реки `Алай` (`на берегу Алая`).

* В тексте сделаны опечатки, которые на первый взгляд не заметны. Например, в слове `Cловарь` первая буква введена латиницей. Если в поисковом запросе ввести слово `Словарь` кириллицей, то документ, в котором указано слово с ошибкой, не будет найден.

Как работает поиск при использовании фильтра `yandex-lemmer`:

1. {{ OS }} анализирует текст запроса при помощи поисковой библиотеки [Apache Lucene](https://lucene.apache.org). В процессе анализа токенизатор разбивает текст на отдельные токены. Токен обычно состоит из слова и метаданных об этом слове. Например, токенизатор может преобразовать предложение `Один в поле не воин` в набор токенов, ориентируясь на пробелы между словами: `Один`, `в`, `поле`, `не`, `воин`.

1. Набор токенов проходит через цепочку фильтров. Фильтр получает токен, анализирует его и возвращает один или несколько токенов. Например, фильтр, который приводит текст к нижнему регистру, вернет то же количество токенов, что и на входе: `один`, `в`, `поле`, `не`, `воин`. Также на выходе токен может отсутствовать, если применен фильтр по стоп-словам.

1. Когда токен попадает в фильтр `yandex_lemmer`, выполняется поиск начальной формы слова. Если найдена одна или несколько начальных форм (например, для омонимов или омографов), то фильтр сначала возвращает исходный токен, а затем возвращает токены с начальными формами слова.

    {% cut "Пример выходных токенов для фразы `идут дожди`" %}

    ```bash
    {
      "tokens": [
        {
          "token": "идут",
          "start_offset": 0,
          "end_offset": 4,
          "type": "<ALPHANUM>",
          "position": 0
        },
        {
          "token": "идти",
          "start_offset": 0,
          "end_offset": 4,
          "type": "<ALPHANUM>",
          "position": 0
        },
        {
          "token": "дожди",
          "start_offset": 5,
          "end_offset": 10,
          "type": "<ALPHANUM>",
          "position": 1
        },
        {
          "token": "дождь",
          "start_offset": 5,
          "end_offset": 10,
          "type": "<ALPHANUM>",
          "position": 1
        }
      ]
    }
    ```

    {% endcut %}

    В приведенном примере при поиске найдутся фразы, которые содержат любую форму слова `идти` (например, `шел`, `шли`, `идут`) и любую форму слова `дождь` (например, `дожди`, `дождей`).

    Каждому токену на выходе присваивается позиция, которая показывает положение слова в поисковой фразе. Если для входящего токена на выходе получается несколько токенов, то у них всех будет одинаковая позиция. Она нужна, чтобы определять расстояние между искомыми словами, например, для запросов в кавычках. Так, при поиске фразы `"идут дожди"` будет найден документ, который содержит фразу `идет дождь`, но не будет найдена фраза `идет сильный дождь`.

## Примеры использования {#examples}

* [{#T}](../tutorials/opensearch-yandex-lemmer.md)
* [{#T}](../tutorials/migration-to-opensearch.md)
